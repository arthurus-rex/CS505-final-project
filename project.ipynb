{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d6d520-4717-423f-b5cc-8d1e77947da4",
   "metadata": {},
   "source": [
    "# CS505 Final Project\n",
    "#### A solo project by Arthur Savage - U02936938\n",
    "\n",
    "The premise for this project is to \n",
    "\n",
    "Do some stuuuuufffffff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7106789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (2.4.0)\n",
      "Requirement already satisfied: packaging in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: importlib-metadata in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (4.8.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: pandas in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: multiprocess in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: responses<0.19 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (0.17.0)\n",
      "Requirement already satisfied: dataclasses in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: filelock in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.1)\n",
      "Requirement already satisfied: pyyaml in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from packaging->datasets) (3.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: six in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from responses<0.19->datasets) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from tqdm>=4.62.1->datasets) (5.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from aiohttp->datasets) (1.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 7.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: importlib-metadata in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from transformers) (4.8.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: requests in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "     |████████████████████████████████| 880 kB 102.8 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "     |████████████████████████████████| 6.6 MB 98.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.8.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\n",
      "     |████████████████████████████████| 759 kB 96.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from tqdm>=4.27->transformers) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: six in /projectnb/perchash/venvs/nlp2/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting click\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 93.6 MB/s            \n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "     |████████████████████████████████| 309 kB 97.3 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=6cda933646b9882201eb28da7879e0aa292c32f6d516dbd08126d2a3160d654e\n",
      "  Stored in directory: /scratch/3154247.1.ece/pip-ephem-wheel-cache-nfclx_zg/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, joblib, click, tokenizers, sacremoses, transformers\n",
      "Successfully installed click-8.0.4 joblib-1.1.1 regex-2023.8.8 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47d3af4-1b33-4bca-897e-57daf829341d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'huggingface'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, list_datasets\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'huggingface'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import datasets\n",
    "from datasets import load_dataset, list_datasets\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split,Dataset,DataLoader\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0b622-02a1-4505-a04a-9e050d3a4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "Dataset.cleanup_cache_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751f8db-c313-425f-8607-a39ebcca695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2', device=0)\n",
    "outputs = generator('I wonder what I will generate?')\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2e5d1-8264-4356-9460-bb319eddadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing GPT-2 model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', device=0)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092703d-3194-450b-8ad0-47fb967d41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and tokenize dataset\n",
    "def encode(batch): return tokenizer([x.strip('@user') for x in batch['tweet']], truncation=True, padding=True)\n",
    "\n",
    "fake_news = load_dataset('tweets_hate_speech_detection', split='train')\n",
    "processed = fake_news.map(encode, batched=True)\n",
    "processed.set_format('torch', columns=['input_ids', 'attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbbf40-0b48-4371-aad6-412e0666df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/projectnb/cs505ws/students/savageaf/project/fake_news/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                 # gpt-2\n",
    "    tokenizer=tokenizer, \n",
    "    args=training_args,          # \n",
    "    data_collator=data_collator, # tells Trainer not to look for vector of labels\n",
    "    train_dataset=processed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946824c2-baff-4a6c-a634-f008e3a2131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune model on hate speech tweets]\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.set_max_workspace_size(max_split_size_mb * 1024 * 1024)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc11ab-b2ce-417e-9150-1f6d186509fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./hate_tweet_model')\n",
    "hate_tweets = pipeline('text-generation', model='./hate_tweet_model', device=0)\n",
    "regular_gpt2 = pipeline('text-generation', model='gpt2', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32806f6a-dddd-434c-b971-7ae839b38590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating text and comparing models\n",
    "print(\"REGULAR GPT-2:\")\n",
    "print(regular_gpt2('It makes me mad when'))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"HATE TWEETS MODEL:\")\n",
    "print(hate_tweets('It makes me mad when'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0e8e6-e6de-4b7e-bcb7-816cbec60410",
   "metadata": {},
   "source": [
    "### NOW WE TRY MODEL TWO\n",
    "\n",
    "iT DOSE oTHEer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095e6c6-1dcb-4e7d-b50d-9f76d4560514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(batch): return tokenizer([x for x in batch['text']], truncation=True, padding=True)\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv('/project/perchash/WS/project/Fake.csv')\n",
    "fake_news = Dataset.from_pandas(df)\n",
    "processed = fake_news.map(encode, batched=True)\n",
    "processed.set_format('torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb643e3-57b5-42ca-ae2f-2844b039a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing second trainer\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir='/projectnb/cs505ws/students/savageaf/project/fake_news2/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model,                 # gpt-2\n",
    "    tokenizer=tokenizer, \n",
    "    args=training_args2,          # \n",
    "    data_collator=data_collator, # tells Trainer not to look for vector of labels\n",
    "    train_dataset=processed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b980024-6aa9-493a-8e8c-c7922d238cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.set_max_workspace_size(max_split_size_mb * 1024 * 1024)\n",
    "\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d19f49-1d4b-486d-9055-126cdb9b9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "     def __init__(self, data: list[list[int]]):\n",
    "         self.data = []\n",
    "         for d in data:\n",
    "             input_ids = torch.tensor(d, dtype=torch.int64)\n",
    "             attention_mask = torch.ones(len(d), dtype=torch.int64)\n",
    "             self.data.append({'input_ids': input_ids,\n",
    "                  'attention_mask': attention_mask, 'labels': input_ids})\n",
    " \n",
    "     def __len__(self):\n",
    "         return len(self.data)\n",
    " \n",
    "     def __getitem__(self, idx: int):\n",
    "         return self.data[idx]\n",
    "\n",
    "\n",
    "def train_one(model, loader, optimizer):\n",
    "     model.train()\n",
    "     losses = []\n",
    "     for batch in tqdm.tqdm(loader):\n",
    "         for k, v in batch.items():\n",
    "             batch[k] = v.to(DEVICE)\n",
    "         optimizer.zero_grad()\n",
    "         out = model(input_ids=batch['input_ids'],\n",
    "                     attention_mask=batch['attention_mask'],\n",
    "                     labels=batch['labels'])\n",
    "         loss = out['loss']\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         losses.append(loss.item())\n",
    "\n",
    "     return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605299d2-13bd-40ae-8d73-abaf7af15a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# calling Dataset constructor and splitting dataset\n",
    "full_dataset = CustomDataset(X,Y)\n",
    "training_dataset, validation_dataset, testing_dataset = random_split(full_dataset, [0.7,0.1,0.2])\n",
    "\n",
    "# creating DataLoader objects of the training, validation, and testing datasets\n",
    "training_dataloader =   DataLoader(training_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_dataset,   batch_size=batch_size, shuffle=False)\n",
    "testing_dataloader =    DataLoader(testing_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "n_train = len(training_dataloader.dataset)\n",
    "n_val = len(validation_dataloader.dataset)\n",
    "n_test = len(testing_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa55835-8a03-4c64-abed-abed70e76d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader_train = torch.utils.data.DataLoader(training_dataset, batch_size=1)\n",
    "loader_val = torch.utils.data.DataLoader(validation_dataset, batch_size=1)\n",
    "\n",
    "DEVICE = 'cuda'    # or 'cpu'\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for i_epoch in range(20):\n",
    "     loss_train = train_one(model, loader_train, optimizer)\n",
    "     loss_val = val_one(model, loader_val)\n",
    "     print(f'{i_epoch} : loss_train={loss_train}, loss_val={loss_val}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3dec32-0acc-4a1a-931b-5b5d5b4b55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "#from google.colab import drive\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8110e4b-9402-40cc-ad84-21931330e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/project/perchash/WS/project/Fake.csv' # fake news CSV from Kaggle\n",
    "df = pd.read_csv(filename) # open CSV\n",
    "df = df['text'] # only select text\n",
    "\n",
    "sentences = []\n",
    "for index in df.index: # iterate through each row of dataframe\n",
    "    document = df[index] # go document by document\n",
    "    split_document = document.split('.') # splits sentences of document by period\n",
    "    sentences = sentences + split_document # concatenates all sentences into a single list of sentences\n",
    "\n",
    "sentences = sentences[:2000] # cutting down dataset size so I finish on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87e392-ee40-4011-83fd-22076b03a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from HW6\n",
    "doc_lengths = []\n",
    "\n",
    "for doc in sentences:\n",
    "\n",
    "    # get rough token count distribution\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "\n",
    "    doc_lengths.append(len(tokens))\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "\n",
    "sns.distplot(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe8c90-1590-4992-9b05-c576632fbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "\n",
    "    for txt in txt_list:\n",
    "\n",
    "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0544e-22d5-4d84-b9cf-03806a10f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "batch_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5b89d-1605-49ca-86e0-279275211fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GPT2Dataset(sentences, tokenizer, max_length=768)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb2a48-77dc-4144-8758-fd6c7e53c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee87b2-8eef-4539-9776-a50a1eca5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not really doing anything with the config buheret\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "model.cuda()\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804af2f-9b97-47dc-ba42-33207243985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters I cooked up that work reasonably well\n",
    "\n",
    "epochs = 4\n",
    "learning_rate = 5e-4\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30797c-b710-4f06-b442-a5b241a37d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba88fb-be61-4e55-bc22-84a13c8bd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a416b00-3a33-4b34-a50b-ee511f6b6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5260d-e886-4183-82f2-fb06b9d5da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(  b_input_ids,\n",
    "                          labels=b_labels,\n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            sample_outputs = model.generate(\n",
    "                                    bos_token_id=random.randint(1,30000),\n",
    "                                    do_sample=True,\n",
    "                                    top_k=50,\n",
    "                                    max_length = 200,\n",
    "                                    top_p=0.95,\n",
    "                                    num_return_sequences=1\n",
    "                                )\n",
    "            for i, sample_output in enumerate(sample_outputs):\n",
    "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs  = model(b_input_ids,\n",
    "#                            token_type_ids=None,\n",
    "                             attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd191e-1d2c-49d2-88c2-48939f11aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display floats with two decimal places.\n",
    "#pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da9426-7532-42b4-8ab5-efda54baff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf210d8-b8d0-4e5c-85da-6b78ded3763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a7158-a377-48c4-ba67-4a51df448828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"<|startoftext|>\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "print(generated)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated,\n",
    "                                #bos_token_id=random.randint(1,30000),\n",
    "                                do_sample=True,\n",
    "                                top_k=50,\n",
    "                                max_length = 300,\n",
    "                                top_p=0.95,\n",
    "                                num_return_sequences=10\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa147e58-3543-49c7-b2c7-84c39316ec9d",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "1. https://www.it-jim.com/blog/training-and-fine-tuning-gpt-2-and-gpt-3-models-using-hugging-face-transformers-and-openai-api/\n",
    "2. https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787\n",
    "3. https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset/\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f869419-ad52-4c6e-be14-877175e8876d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
